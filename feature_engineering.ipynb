{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace, col, udf\nfrom pyspark.sql.types import IntegerType, TimestampType\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import Pipeline\nimport plotly.graph_objects as go\nimport plotly\nimport datetime\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["\ndef timestamp_transform(x):\n  return datetime.datetime.fromtimestamp(x).hour\n#spark.udf.register(\"timestamp_transform\", timestamp_transform)\nformat_timestamp_udf = udf(lambda x: timestamp_transform(x))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["def cat(cat_list, df):\n  # The index of string vlaues multiple columns\n  print(\"categorized varibales function begin\")\n  indexers = [\n    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n    for c in cat_list\n  ]\n  \n  # The encode of indexed vlaues multiple columns\n  encoders = [OneHotEncoder(dropLast=True,inputCol=indexer.getOutputCol(),\n              outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n      for indexer in indexers\n  ]\n\n  # Vectorizing encoded values\n  #assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"cat_features\")\n  cols =[encoder.getOutputCol() for encoder in encoders]\n  pipeline = Pipeline(stages=indexers+ encoders)\n  model=pipeline.fit(df)\n  transformed_cat = model.transform(df).select(cols)\n  \n  \n  return transformed_cat"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["def text_processing(df):\n  ''' Regex Tokenizer removes the punctuation and tokenizes the text\n  StopWordsRemover to remove stopwords --default list of stopwords from english library\n  HashingTF counts the word frequency but with consums lesser memory as it hashes the frequency\n  Word2Vec produces word embedding'''\n  tok = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\") \n  remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  #tok = Tokenizer(inputCol=\"title\", outputCol=\"words\") \n  htf = HashingTF(inputCol=\"filtered\", outputCol=\"tf\", numFeatures=200) \n  w2v = Word2Vec(inputCol=\"filtered\", outputCol=\"w2v\")\n  \n  pipeline = Pipeline(stages=[tok, remover,htf, w2v]) \n  # Fit the pipeline \n  model = pipeline.fit(df)  \n  \n  #choosing one feature out of HTF and word2vec\n  transform_text = model.transform(df).select(\"w2v\")\n  print(\"text transform done\")\n  return transform_text"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["spark.conf.set( \"spark.sql.crossJoin.enabled\" , \"true\" )"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def features(df, cat_cols, int_cols):\n  #changing created_utc to hours of the day\n  df = df.withColumn(\"created_utc\", col(\"created_utc\").cast(\"integer\"))\n  \n  #calling timestamp_transform function\n  df = df.withColumn('hour_of_day', format_timestamp_udf(df['created_utc']))\n  df = df.drop('created_utc')\n  \n  #now hours of the day can be one-hot encoded as a feature, appending it in the list of categorical variables\n  \n  cat_cols.append(\"hour_of_day\")\n  \n  \n  #encoding cateorical columns\n  transformed_cat = cat(cat_cols, df)\n  \n  #transforming text to word embeddings\n  transfomed_text = text_processing(df)\n  \n  #integer features\n  integer_features = df.select(int_cols)\n  \n  label = df.select('score')\n  \n  #combining all the variables in a dataframe\n  output_df = transformed_cat.join(transfomed_text,how = \"outer\")\n  \n  output_df1 = output_df.join(integer_features,how =  \"outer\")\n  \n  input_cols = output_df1.columns\n  #scaling using StandardScaler and combining using vector assembler as an output column - feature\n  print(\"VA begins\")\n  \n  va = VectorAssembler(inputCols = input_cols , outputCol = \"features\")\n  output_df2 = va.transform(output_df1)\n  print(\"VA done\")\n  scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeature\",\n                        withStd=True, withMean=True)\n  model = scaler.fit(output_df)\n  output_df3 = model.transform(output_df2).select(\"scaledFeatures\")\n  print(\"Scaling Done\")\n  '''\n  pipeline = Pipeline(stages =va+scaler)\n  model = pipeline.fit(output_df)\n  output_df = model.transform(output_df).select(\"scaledFeatures\")'''\n  print(\"VA done\")\n  output_df4 = output_df3.join(label)\n  \n  return output_df4"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"feature_engineering","notebookId":3934637372401901},"nbformat":4,"nbformat_minor":0}
